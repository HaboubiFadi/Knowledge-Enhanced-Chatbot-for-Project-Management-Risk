{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123e9f17",
   "metadata": {},
   "source": [
    "## start with Creating tf vectorizer\n",
    "   ### Require creating pre processing pipeline that contane (remove stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4bd548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Haboubi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Haboubi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Haboubi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lowercasing(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "def join_token(token):\n",
    "    return ' '.join(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6176c84",
   "metadata": {},
   "source": [
    "#### *Create preprocessing pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9203c3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quick brown fox jump lazy dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing_pipeline(txt):\n",
    "    lower=lowercasing(txt)\n",
    "    remove_punc=remove_punctuation(lower)\n",
    "    token=tokenize(remove_punc)\n",
    "    remove_stop=remove_stopwords(token)\n",
    "    lemma=lemmatize(remove_stop)\n",
    "    return join_token(lemma)\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "preprocessing_pipeline(sample_text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07b734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31622777 0.31622777 0.31622777 0.63245553 0.31622777 0.31622777\n",
      "  0.31622777]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample data\n",
    "corpus = 'This is the first document . This document is the second document And this is the third one. Is this the first document?'\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "new_text = \"This document discusses various aspects of natural language processing.\"\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([preprocessing_pipeline(corpus)])\n",
    "new_text_tfidf = tfidf_vectorizer.transform([preprocessing_pipeline(new_text)])\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "def tfidf_matrix(text):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([preprocessing_pipeline(text)])\n",
    "    return tfidf_vectorizer\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd74f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c806598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_keywords(tfidf_vectorizer, text, top_n=5):\n",
    "    tfidf_scores = tfidf_vectorizer.transform([preprocessing_pipeline(text)])\n",
    "    feature_names=tfidf_vectorizer.get_feature_names_out()\n",
    "    print(tfidf_scores)\n",
    "\n",
    "    top_indices = tfidf_scores.argsort()[0, ::-1][:top_n]\n",
    "    \n",
    "    keywords = [feature_names[i] for i in top_indices]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85036516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "# Extract text with PyPDF2\n",
    "from PyPDF2 import PdfReader,PdfWriter\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.layout import LAParams\n",
    "import pdfplumber\n",
    "import fitz\n",
    "pdf_name='PMBOK6-2017.pdf'\n",
    "\n",
    "def extract_text_from_pdf_PyPDF2(pdf_name,debut=0,end=0):\n",
    "    text=''\n",
    "    pdf_file=open(pdf_name,'rb')\n",
    "    pdf=PyPDF2.PdfReader(pdf_file)\n",
    "    if end==0:\n",
    "        end=len(pdf.pages)-1\n",
    "    for pg_num in range(debut,end+1):\n",
    "        page=pdf.pages[pg_num]\n",
    "        text += page.extract_text ()\n",
    "    return  text   \n",
    "# Extract selected pages as pdf\n",
    "def get_selected_pages_as_pdf(pdf_path,start,end):\n",
    "    with open(pdf_path,'rb') as f:\n",
    "        all_pdf=PdfReader(f)\n",
    "        write_pdf=PdfWriter()\n",
    "        new_pdf_name=f\"{pdf_path}_from_page_{start}_{end}.pdf\"\n",
    "        if end!=len(all_pdf.pages):\n",
    "            end+=1\n",
    "        \n",
    "        for page_num in range(start-1,end-1):\n",
    "            pages=all_pdf.pages[page_num]\n",
    "            write_pdf.add_page(pages)\n",
    "        with open (new_pdf_name,'wb') as out:\n",
    "            write_pdf.write(out)    \n",
    "\n",
    "# Extract text with  pdfminer\n",
    "def extract_text_from_pdf_pdfminer(pdf_path):\n",
    "    laparams = LAParams()\n",
    "    text = extract_text(pdf_path,laparams=laparams)\n",
    "    return text\n",
    "\n",
    "def organaize_title_with_sub_title(liste_titre,liste_titre_sub):\n",
    "    dic_titre_position={}\n",
    "    dic_titre={}\n",
    "    liste_titre_position=[]\n",
    "    liste_only_titre=[]\n",
    "    for index1 in range(len(liste_titre)) :\n",
    "        for index in range(len(liste_titre_sub)):\n",
    "            space=liste_titre[index1][0].find(' ')\n",
    "            if liste_titre_sub[index][0].find(liste_titre[index1][0][:space])>=0:\n",
    "                liste_titre_position.append(liste_titre_sub[index])\n",
    "                \n",
    "                liste_only_titre.append(liste_titre_sub[index][0])\n",
    "        \n",
    "        dic_titre_position[liste_titre[index1]]=liste_titre_position\n",
    "        \n",
    "        dic_titre[liste_titre[index1][0]]=liste_only_titre\n",
    "        \n",
    "        liste_titre_position=[]\n",
    "        \n",
    "        liste_only_titre=[]\n",
    "        \n",
    "    return dic_titre,dic_titre_position\n",
    "def sort_liste_based_on_element(liste,element):\n",
    "        sorted_list=sorted(liste,key=lambda x:x[element])\n",
    "        return sorted_list\n",
    "    \n",
    "\n",
    "def get_liste_of_slice(dic_position_liste,last_end=-1): #require a dictionary that contains the position of each title\n",
    "    dic_contenu={}\n",
    "    liste=[]\n",
    "    dic_position_liste=list(dic_position_liste)\n",
    "    dic_position_liste=sort_liste_based_on_element(dic_position_liste,1)#sort liste_element based on second element example ('11.1',102)>('11.2',106)\n",
    "    for i in range(len(dic_position_liste)):\n",
    "        start_position=int(dic_position_liste[i][1])\n",
    "        if i!=len(dic_position_liste)-1:\n",
    "            end_position=int(dic_position_liste[i+1][1])\n",
    "        else:\n",
    "            end_position=last_end\n",
    "        liste.append([start_position,end_position])\n",
    "    \n",
    "         \n",
    "        \n",
    "    return liste\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_contenu(dic_position,text): # require a dictionary that contains the position of each title\n",
    "    dic_contenu={}\n",
    "    liste_slice=get_liste_of_slice(dic_position)\n",
    "    print(liste_slice)\n",
    "    for i in range(len(liste_slice)):\n",
    "        start_position=liste_slice[i][0]\n",
    "        end_position=liste_slice[i][1]\n",
    "        dic_contenu[list(dic_position.keys())[i][0]]=text[start_position:end_position]\n",
    "    return dic_contenu\n",
    "\n",
    "def get_detailed_contenu(dic_position_title,text): # require same as get_contenu , but it dig deeper and get content of sub_titles\n",
    "    \n",
    "    liste_of_keys=list(dic_position_title.keys())\n",
    "    dic_contenu={}\n",
    "    \n",
    "    for index in range(len(liste_of_keys)):\n",
    "        \n",
    "        liste_value_per_key=dic_position_title[liste_of_keys[index]]\n",
    "        if index <=len(liste_of_keys)-2:\n",
    "            last_position=liste_of_keys[index+1][1]\n",
    "        else:\n",
    "            last_position=-1\n",
    "        liste_slice_of_value=get_liste_of_slice(liste_value_per_key,last_position)\n",
    "            \n",
    "        for i in range(len(liste_slice_of_value)):\n",
    "            start_position=liste_slice_of_value[i][0]\n",
    "            end_position=liste_slice_of_value[i][1]\n",
    "            dic_contenu[liste_value_per_key[i][0]]=text[start_position:end_position]\n",
    "    return dic_contenu    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Extract tables using pdfplumber\n",
    "\n",
    "def extract_tables_pdfplumber(pdf_path):\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number in range(len(pdf.pages)):\n",
    "            page = pdf.pages[page_number]\n",
    "            page_tables = page.extract_tables()\n",
    "            if page_tables:\n",
    "                for table in page_tables:\n",
    "                    tables.append(table)\n",
    "    \n",
    "    return tables\n",
    "    return tables    \n",
    "    \n",
    "    \n",
    "def extract_text_from_pdf_usingfitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "\n",
    "    doc.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938a5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf vectorizer on extracted data\n",
    "\n",
    "intro_name='PMBOK6-2017.pdf_from_page_431_436.pdf'\n",
    "chapiter_name='PMBOK6-2017.pdf_from_page_437_494.pdf'\n",
    "# Extract text from new selected pages {chapiter_name}\n",
    "extracted_text = extract_text_from_pdf_usingfitz(chapiter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19caa9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid=tfidf_matrix(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9cdfdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5479)\t0.20851441405707477\n",
      "  (0, 5052)\t0.20851441405707477\n",
      "  (0, 4874)\t0.20851441405707477\n",
      "  (0, 4790)\t0.20851441405707477\n",
      "  (0, 3945)\t0.20851441405707477\n",
      "  (0, 3750)\t0.20851441405707477\n",
      "  (0, 3556)\t0.20851441405707477\n",
      "  (0, 3477)\t0.20851441405707477\n",
      "  (0, 2991)\t0.20851441405707477\n",
      "  (0, 2872)\t0.20851441405707477\n",
      "  (0, 2779)\t0.20851441405707477\n",
      "  (0, 2734)\t0.20851441405707477\n",
      "  (0, 2513)\t0.20851441405707477\n",
      "  (0, 2121)\t0.20851441405707477\n",
      "  (0, 2119)\t0.20851441405707477\n",
      "  (0, 1918)\t0.41702882811414954\n",
      "  (0, 1909)\t0.41702882811414954\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "argsort not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8292\\1367327818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ENTERPRISE ENVIRONMENTAL FACTORS The enterprise environmental factors that can inﬂuence the Plan Risk Management process include but are not limited to overall risk thresholds set by the organization or key stakeholders .'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextract_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8292\\3424719396.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[1;34m(tfidf_vectorizer, text, top_n)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtop_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: argsort not found"
     ]
    }
   ],
   "source": [
    "text_f='ENTERPRISE ENVIRONMENTAL FACTORS The enterprise environmental factors that can inﬂuence the Plan Risk Management process include but are not limited to overall risk thresholds set by the organization or key stakeholders .'\n",
    "f=extract_keywords(tfid,text_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf43e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Load the spaCy English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract noun phrases (entities)\n",
    "    entities = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "text = \"The enterprise environmental factors that can influence the Plan Risk Management process include but are not limited to overall risk thresholds set by the organization or key stakeholders.\"\n",
    "entities = extract_entities(text)\n",
    "print(\"Entities:\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a172b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_words_relationhsip(txt,relationship='influence',verb=True):\n",
    "    ADP=False\n",
    "    liste_word=[]\n",
    "    word=''\n",
    "    txt=normalize_text(txt)\n",
    "    txt=txt.replace('\\n','')\n",
    "    doc = nlp(txt)\n",
    "    save_key_sentence=False\n",
    "    still_word=False\n",
    "    \n",
    "    \n",
    "    print(txt)\n",
    "    for token in doc :\n",
    "        \n",
    "        if( token.pos_ == 'VERB' and token.text=='influence') or (token.text=='components'):\n",
    "            ADP=True\n",
    "        if ADP==True and token.pos_ == 'ADP':\n",
    "            \n",
    "            save_key_sentence=True\n",
    "        if save_key_sentence==True or still_word==True:\n",
    "            if token.pos_!='PRON' and token.pos_!='PUNCT ' :\n",
    "                \n",
    "                word=word+' '+token.text\n",
    "                print(word)\n",
    "            \n",
    "            else:\n",
    "                save_key_sentence==False\n",
    "                liste_word.append(word)\n",
    "                word=''\n",
    "    if word !='':\n",
    "        liste_word.append(word)\n",
    "    return liste_word  \n",
    "\n",
    "def verify_mutiple_input(text):\n",
    "    if ':' in normalize_text(text):\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    "\n",
    "def Extract_inputs_sub_process(text):\n",
    "    input_string='considered as inputs'\n",
    "    if input_string in text:\n",
    "        print('Extract input')\n",
    "\n",
    "        if verify_mutiple_input(text):\n",
    "            input_liste=extract_key_word_content(text)[0]\n",
    "            print(input_liste)\n",
    "            return input_liste\n",
    "        \n",
    "    return ''\n",
    "def Extract_components_sub_process(text):\n",
    "    components_string='components'\n",
    "\n",
    "    if components_string in text :\n",
    "        print('Extract components')\n",
    "\n",
    "        if verify_mutiple_input(text):\n",
    "            input_liste=extract_key_word_content(text)[0]\n",
    "            \n",
    "            if input_liste ==[]:\n",
    "                input_liste=extract_key_words_relationhsip(text,'components')\n",
    "            print(input_liste)\n",
    "    \n",
    "            return input_liste\n",
    "        \n",
    "    return None\n",
    "\n",
    "def Extract_technologies_sub_process(text):\n",
    "    components_string='techniques'\n",
    "\n",
    "    if components_string in text and 'include' in text:\n",
    "        print('Extract techniques')\n",
    "\n",
    "        if verify_mutiple_input(text):\n",
    "            input_liste=extract_key_word_content(text)[0]\n",
    "            print(input_liste)\n",
    "            \n",
    "            return input_liste\n",
    "    return None    \n",
    "def Extract_Figure(text):\n",
    "    regex=r'Figure \\d+-\\d+'\n",
    "    pattern=re.compile(regex)\n",
    "    liste=[]\n",
    "    matchs=pattern.finditer(text)\n",
    "    for match in matchs:\n",
    "        liste.append(match.group())\n",
    "    return set(liste)\n",
    "\n",
    "def Extract_Table(text):\n",
    "    regex=r'Table \\d+-\\d+'\n",
    "    pattern=re.compile(regex)\n",
    "    liste=[]\n",
    "    matchs=pattern.finditer(text)\n",
    "    for match in matchs:\n",
    "        liste.append(match.group())\n",
    "    return set(liste)        \n",
    "    return None\n",
    "\n",
    "def Extract_influence_relationship(text):\n",
    "    influence_string='that can influence'\n",
    "\n",
    "    if influence_string in normalize_text(text) :\n",
    "        print('Extract influence')\n",
    "\n",
    "        if verify_mutiple_input(text):\n",
    "            input_liste=extract_key_word_content_v2(text)\n",
    "            if input_liste ==[]:\n",
    "                input_liste=extract_key_words_relationhsip(text)\n",
    "                print(input_liste)\n",
    "\n",
    "            print(input_liste)\n",
    "            \n",
    "            return input_liste\n",
    "    return None\n",
    "\n",
    "# Extract key_words:\n",
    "def extract_key_word_content(txt):\n",
    "    #regex=r'(uu).*(\\.\\n)' regex using text extracted from pypdf\n",
    "    regex=r\"u\\nu.*?(\\.)\"\n",
    "    pattern=re.compile(regex)\n",
    "    matchs=pattern.finditer(txt)\n",
    "    key_word=[]\n",
    "    detailed={}\n",
    "    for match in matchs:\n",
    "        string= match.group()\n",
    "        if string.find('.')>0 and string.find('.')<len(string):\n",
    "            \n",
    "            detail_debut=match.span()[1]\n",
    "            detail_end=txt[detail_debut:].find('.\\n')\n",
    "            key=string[:match.group().find('.')].replace('u\\nu','')\n",
    "            key_word.append(key)\n",
    "            detailed[key]=txt[detail_debut:detail_debut+detail_end]\n",
    "            \n",
    "        else:\n",
    "            key_word.append(string)\n",
    "    return key_word,detailed\n",
    "\n",
    "def extract_key_word_content_v2(txt):\n",
    "    #regex=r'(uu).*(\\.\\n)' regex using text extracted from pypdf\n",
    "    regex=r\"u\\nu.*?(\\.|,)\"\n",
    "    pattern=re.compile(regex)\n",
    "    matchs=pattern.finditer(txt)\n",
    "    key_word=[]\n",
    "    detailed={}\n",
    "    for match in matchs:\n",
    "        string= match.group()\n",
    "        \n",
    "            \n",
    "            \n",
    "        key_word.append(string.replace('u\\nu',''))\n",
    "            \n",
    "        \n",
    "    return key_word        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12f20340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def filter_single_word_entities(entity_dict):\n",
    "    filtered_entities = {entity: verb for entity, verb in entity_dict.items() if len(entity.split()) > 1}\n",
    "    return filtered_entities\n",
    "\n",
    "\n",
    "def remove_stopwords_from_entities(entity_dict):\n",
    "    new_dic={}\n",
    "    for entites,relation in entity_dict.items():\n",
    "        print(entites)\n",
    "        new_key=join_token(remove_stopwords(tokenize(entites)))\n",
    "        print(new_key)\n",
    "        new_dic[new_key]=relation\n",
    "    return new_dic   \n",
    "def clean_entites_verb(entity_verbs):\n",
    "    entity_verbs_news={}\n",
    "    for key,val in  entity_verbs.items():\n",
    "        if isinstance(val, list)==True:\n",
    "            entity_verbs_news[key]=val[0]\n",
    "        else:\n",
    "            entity_verbs_news[key]=val\n",
    "    return entity_verbs_news \n",
    "def extract_entities_with_relationship(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entity_verbs = {}\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        entity = chunk.text\n",
    "         \n",
    "        main_verb = None\n",
    "        for token in chunk.root.ancestors:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                main_verb = token.text\n",
    "                break\n",
    "        for token in chunk.root.children:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                main_verb = token.text\n",
    "                break        \n",
    "        \n",
    "        if main_verb:\n",
    "            entity_verbs[entity] = main_verb\n",
    "    return clean_entites_verb(entity_verbs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3889e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dic(entites,extracted,relatonship):\n",
    "    if extracted==None:\n",
    "        return entites\n",
    "    for e in extracted:\n",
    "        entites[e]=relatonship\n",
    "    return entites\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(input_text):\n",
    "    cleaned_text = input_text.replace('u\\nu', ' ').replace('/n','').replace('  ','').strip()\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    cleaned_text=clean_numeration(cleaned_text,3)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "\n",
    "def nlp_pipeline_entites_extraction(text):\n",
    "    text=normalize_text(text)\n",
    "    clean=clean_text(text)\n",
    "    entites_relation={}\n",
    "    # first layer extract :\n",
    "    entites_relation=extract_entities_with_relationship(clean)\n",
    "    print(entites_relation)\n",
    "    # second layer extract influence_rela\n",
    "    extracted_influence=Extract_influence_relationship(text)\n",
    "    entites_relation=convert_dic(entites_relation,extracted_influence,'influence')\n",
    "    \n",
    "    # third layer extract input_relationship\n",
    "    extracted_input=Extract_inputs_sub_process(text)\n",
    "    entites_relation=convert_dic(entites_relation,extracted_input,'input')\n",
    "    # fourth layer extract conponment_relationship\n",
    "    \n",
    "    extracted_components=Extract_components_sub_process(text)\n",
    "    entites_relation=convert_dic(entites_relation,extracted_components,'components')\n",
    "    # Fifth layer extract thecnique_relationship\n",
    "    \n",
    "    extracted_tech=Extract_technologies_sub_process(text)\n",
    "    entites_relation=convert_dic(entites_relation,extracted_tech,'techniques')\n",
    "    return entites_relation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a778a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract txt from corpse\n",
    "#Functions\n",
    "# Extract text with PyPDF2\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader,PdfWriter\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.layout import LAParams\n",
    "import pdfplumber\n",
    "import fitz\n",
    "pdf_name='PMBOK6-2017.pdf'\n",
    "\n",
    "def extract_text_from_pdf_PyPDF2(pdf_name,debut=0,end=0):\n",
    "    text=''\n",
    "    pdf_file=open(pdf_name,'rb')\n",
    "    pdf=PyPDF2.PdfReader(pdf_file)\n",
    "    if end==0:\n",
    "        end=len(pdf.pages)-1\n",
    "    for pg_num in range(debut,end+1):\n",
    "        page=pdf.pages[pg_num]\n",
    "        text += page.extract_text ()\n",
    "    return  text   \n",
    "# Extract selected pages as pdf\n",
    "def get_selected_pages_as_pdf(pdf_path,start,end):\n",
    "    with open(pdf_path,'rb') as f:\n",
    "        all_pdf=PdfReader(f)\n",
    "        write_pdf=PdfWriter()\n",
    "        new_pdf_name=f\"{pdf_path}_from_page_{start}_{end}.pdf\"\n",
    "        if end!=len(all_pdf.pages):\n",
    "            end+=1\n",
    "        \n",
    "        for page_num in range(start-1,end-1):\n",
    "            pages=all_pdf.pages[page_num]\n",
    "            write_pdf.add_page(pages)\n",
    "        with open (new_pdf_name,'wb') as out:\n",
    "            write_pdf.write(out)    \n",
    "\n",
    "# Extract text with  pdfminer\n",
    "def extract_text_from_pdf_pdfminer(pdf_path):\n",
    "    laparams = LAParams()\n",
    "    text = extract_text(pdf_path,laparams=laparams)\n",
    "    return text\n",
    "\n",
    "def organaize_title_with_sub_title(liste_titre,liste_titre_sub):\n",
    "    dic_titre_position={}\n",
    "    dic_titre={}\n",
    "    liste_titre_position=[]\n",
    "    liste_only_titre=[]\n",
    "    for index1 in range(len(liste_titre)) :\n",
    "        for index in range(len(liste_titre_sub)):\n",
    "            space=liste_titre[index1][0].find(' ')\n",
    "            if liste_titre_sub[index][0].find(liste_titre[index1][0][:space])>=0:\n",
    "                liste_titre_position.append(liste_titre_sub[index])\n",
    "                \n",
    "                liste_only_titre.append(liste_titre_sub[index][0])\n",
    "        \n",
    "        dic_titre_position[liste_titre[index1]]=liste_titre_position\n",
    "        \n",
    "        dic_titre[liste_titre[index1][0]]=liste_only_titre\n",
    "        \n",
    "        liste_titre_position=[]\n",
    "        \n",
    "        liste_only_titre=[]\n",
    "        \n",
    "    return dic_titre,dic_titre_position\n",
    "def sort_liste_based_on_element(liste,element):\n",
    "        sorted_list=sorted(liste,key=lambda x:x[element])\n",
    "        return sorted_list\n",
    "    \n",
    "\n",
    "def get_liste_of_slice(dic_position_liste,last_end=-1): #require a dictionary that contains the position of each title\n",
    "    dic_contenu={}\n",
    "    liste=[]\n",
    "    dic_position_liste=list(dic_position_liste)\n",
    "    dic_position_liste=sort_liste_based_on_element(dic_position_liste,1)#sort liste_element based on second element example ('11.1',102)>('11.2',106)\n",
    "    for i in range(len(dic_position_liste)):\n",
    "        start_position=int(dic_position_liste[i][1])\n",
    "        if i!=len(dic_position_liste)-1:\n",
    "            end_position=int(dic_position_liste[i+1][1])\n",
    "        else:\n",
    "            end_position=last_end\n",
    "        liste.append([start_position,end_position])\n",
    "    \n",
    "         \n",
    "        \n",
    "    return liste\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_contenu(dic_position,text): # require a dictionary that contains the position of each title\n",
    "    dic_contenu={}\n",
    "    liste_slice=get_liste_of_slice(dic_position)\n",
    "    print(liste_slice)\n",
    "    for i in range(len(liste_slice)):\n",
    "        start_position=liste_slice[i][0]\n",
    "        end_position=liste_slice[i][1]\n",
    "        dic_contenu[list(dic_position.keys())[i][0]]=text[start_position:end_position]\n",
    "    return dic_contenu\n",
    "\n",
    "def get_detailed_contenu(dic_position_title,text): # require same as get_contenu , but it dig deeper and get content of sub_titles\n",
    "    \n",
    "    liste_of_keys=list(dic_position_title.keys())\n",
    "    dic_contenu={}\n",
    "    \n",
    "    for index in range(len(liste_of_keys)):\n",
    "        \n",
    "        liste_value_per_key=dic_position_title[liste_of_keys[index]]\n",
    "        if index <=len(liste_of_keys)-2:\n",
    "            last_position=liste_of_keys[index+1][1]\n",
    "        else:\n",
    "            last_position=-1\n",
    "        liste_slice_of_value=get_liste_of_slice(liste_value_per_key,last_position)\n",
    "            \n",
    "        for i in range(len(liste_slice_of_value)):\n",
    "            start_position=liste_slice_of_value[i][0]\n",
    "            end_position=liste_slice_of_value[i][1]\n",
    "            dic_contenu[liste_value_per_key[i][0]]=text[start_position:end_position]\n",
    "    return dic_contenu    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Extract tables using pdfplumber\n",
    "\n",
    "def extract_tables_pdfplumber(pdf_path):\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number in range(len(pdf.pages)):\n",
    "            page = pdf.pages[page_number]\n",
    "            page_tables = page.extract_tables()\n",
    "            if page_tables:\n",
    "                for table in page_tables:\n",
    "                    tables.append(table)\n",
    "    \n",
    "    return tables\n",
    "    return tables    \n",
    "    \n",
    "    \n",
    "def extract_text_from_pdf_usingfitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "\n",
    "    doc.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b08ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 PLAN RISK MANAGEMENT\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "regex_title=r\"11(\\.\\d){1}\\ .+\"\n",
    "regex_sub_title=r\"11(\\.\\d){2}\\ .+\"\n",
    "regex_sub2_title=r\"11(\\.\\d){3}\\ .+\" # this for sub_sub_title example : 11.1.1.1\n",
    "all_liste=[]\n",
    "liste_titre=[]\n",
    "liste_sub_title=[]\n",
    "liste_sub2_title=[]\n",
    "for regex in [regex_title,regex_sub_title,regex_sub2_title]:\n",
    "    liste=[]\n",
    "    pattern = re.compile(regex)\n",
    "\n",
    "    matches=pattern.finditer(extracted_text)\n",
    "    for match in matches :\n",
    "        liste.append((match.group(),match.start()))\n",
    "    all_liste.append([liste])\n",
    "liste_titre=all_liste[0][0]\n",
    "liste_sub_title=all_liste[1][0] \n",
    "liste_sub2_title =all_liste[2][0]  \n",
    "all_liste=[]\n",
    "print(liste_titre[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698d7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_title,dic_title_position=organaize_title_with_sub_title(liste_titre,liste_sub_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a7fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sub_title,dic_sub_title_position=organaize_title_with_sub_title(liste_sub_title,liste_sub2_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9cc188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 12474], [12474, 34254], [34254, 51201], [51201, 72011], [72011, 100705], [100705, 108098], [108098, -1]]\n"
     ]
    }
   ],
   "source": [
    "dict_contenu=get_contenu(dic_title_position,extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc8a12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('11.1 PLAN RISK MANAGEMENT', 48),\n",
       " ('11.2 IDENTIFY RISKS', 12474),\n",
       " ('11.3 PERFORM QUALITATIVE RISK ANALYSIS', 34254),\n",
       " ('11.4 PERFORM QUANTITATIVE RISK ANALYSIS', 51201),\n",
       " ('11.5 PLAN RISK RESPONSES', 72011),\n",
       " ('11.6 IMPLEMENT RISK RESPONSES', 100705),\n",
       " ('11.7 MONITOR RISKS', 108098)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_liste_of_slice(dic_title_position.keys())\n",
    "liste=sort_liste_based_on_element(list(dic_title_position.keys()),0)\n",
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce2f2185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.1.1.2 PROJECT MANAGEMENT PLAN\\nDescribed in Section 4.2.3.1. In planning Project Risk Management, all approved subsidiary management plans \\nshould be taken into consideration in order to make the risk management plan consistent with them. The methodology \\noutlined in other project management plan components might inﬂuence the Plan Risk Management process.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_contenu=get_detailed_contenu(dic_sub_title_position,extracted_text)\n",
    "detailed_contenu['11.1.1.2 PROJECT MANAGEMENT PLAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c27b6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def Extract_refenrence(text):\n",
    "    regex=r'Described in Section.+\\.\\s'\n",
    "    pattern=re.compile(regex)\n",
    "    liste_ref=[]\n",
    "    matchs=pattern.finditer(text)\n",
    "    for match in matchs:\n",
    "        liste_ref.append(match.group())\n",
    "    return liste_ref\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, POS tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_actionable_items(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    actionable_items = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        if any(token.pos_ == \"VERB\" for token in sent):\n",
    "            if sent[0].tag_ == \"VB\" or sent[0].tag_ == \"VBG\":\n",
    "                \n",
    "                actionable_items.append(sent.text)\n",
    "    \n",
    "    return actionable_items\n",
    "\n",
    "# Example text\n",
    "example_text = 'make the risk management plan consistent with them. The methodology outlined in other project management plan components might inﬂuence the Plan Risk Management process.'\n",
    "\n",
    "# Extract actionable items\n",
    "actionable_items = extract_actionable_items(example_text)\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "\n",
    "def string_in_liste(string,liste):\n",
    "    statue=False\n",
    "    i=0\n",
    "    while (statue==False and i<len(liste)):\n",
    "        \n",
    "        if  liste[i] in normalize_text(string):\n",
    "            statue=True\n",
    "        else:\n",
    "            i+=1\n",
    "    return statue\n",
    "        \n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "def extract_definition(dictionary,text):\n",
    "    doc = nlp(text)\n",
    "    defintion=[]\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        if any(normalize_text(token.lemma_) in dictionary for token in sent): \n",
    "            defintion.append(sent)\n",
    "            break\n",
    "            \n",
    "    return defintion      \n",
    "\n",
    "def Extract_example(text):\n",
    "    example=[]\n",
    "    regex=r'\\b(for example|such as|include|as an example).*\\.'\n",
    "    pattern=re.compile(regex)\n",
    "    \n",
    "    matchs=pattern.finditer(text)\n",
    "    for match in matchs :\n",
    "        debut=match.span()[0]\n",
    "        end=match.span()[1]\n",
    "        sample=text[:debut]\n",
    "        last_period_mark=sample[::-1].find('.')\n",
    "        if last_period_mark!=-1:\n",
    "            real_debut=debut-last_period_mark\n",
    "        else:\n",
    "            real_debut=debut\n",
    "        example.append(text[real_debut:end])\n",
    "    return example \n",
    "\n",
    "def Extract_Usercases(text):\n",
    "    Usercases=[]\n",
    "  \n",
    "    regex=r'\\b(use case|scenario|application|situation).*\\.'\n",
    "    pattern=re.compile(regex)\n",
    "    \n",
    "    matchs=pattern.finditer(text)\n",
    "    for match in matchs :\n",
    "        debut=match.span()[0]\n",
    "        end=match.span()[1]\n",
    "        sample=text[:debut]\n",
    "        last_period_mark=sample[::-1].find('.')\n",
    "        if last_period_mark!=-1:\n",
    "            real_debut=debut-last_period_mark\n",
    "        else:\n",
    "            real_debut=debut\n",
    "        Usercases.append(text[real_debut:end])\n",
    "    return Usercases   \n",
    "def string_in_liste(string,liste):\n",
    "    statue=False\n",
    "    i=0\n",
    "    while (statue==False and i<len(liste)):\n",
    "        \n",
    "        if  liste[i] in normalize_text(string):\n",
    "            statue=True\n",
    "        else:\n",
    "            i+=1\n",
    "    return statue\n",
    "def parent_process_relationship(dic_title,txt):\n",
    "    for key,value in dic_title.items():\n",
    "        if txt in value:\n",
    "            parent,relationship=key.split(':')\n",
    "            return clean_numeration(parent,2),relationship \n",
    "def clean_numeration(text,level=1):\n",
    "    regex_title=r\"11(\\.\\d){1}\\s\"\n",
    "    regex_sub_title=r\"11(\\.\\d){2}\\s\"\n",
    "    regex_sub2_title=r\"11(\\.\\d){3}\\s\" # this for sub_sub_title example : 11.1.1.1\n",
    "    if level==1:\n",
    "        regex=regex_title\n",
    "        pattern = re.compile(regex)\n",
    "\n",
    "        matches=pattern.finditer(text)\n",
    "        for m in matches:\n",
    "            return text.replace(m.group(),'')\n",
    "    if level==2:\n",
    "        regex=regex_sub_title\n",
    "        \n",
    "        pattern = re.compile(regex)\n",
    "\n",
    "        matches=pattern.finditer(text)\n",
    "        for m in matches:\n",
    "            return text.replace(m.group(),'')\n",
    "    if level==3:\n",
    "        regex=regex_sub2_title\n",
    "   \n",
    "        pattern = re.compile(regex)\n",
    "\n",
    "        matches=pattern.finditer(text)\n",
    "        for m in matches:\n",
    "            return text.replace(m.group(),'')         \n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "def extract_definition(dictionary,text):\n",
    "    doc = nlp(text)\n",
    "    defintion=[]\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        if any(normalize_text(token.lemma_) in dictionary for token in sent): \n",
    "            defintion.append(sent)\n",
    "            break\n",
    "            \n",
    "    return defintion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42e9a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe_contenu=pd.DataFrame(detailed_contenu.items(),columns=['sub_title','contenu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be77aae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_title</th>\n",
       "      <th>contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.1.1.1 PROJECT CHARTER</td>\n",
       "      <td>11.1.1.1 PROJECT CHARTER\\nDescribed in Section...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.1.1.2 PROJECT MANAGEMENT PLAN</td>\n",
       "      <td>11.1.1.2 PROJECT MANAGEMENT PLAN\\nDescribed in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.1.1.3 PROJECT DOCUMENTS</td>\n",
       "      <td>11.1.1.3 PROJECT DOCUMENTS\\nProject documents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.1.1.4 ENTERPRISE ENVIRONMENTAL FACTORS</td>\n",
       "      <td>11.1.1.4 ENTERPRISE ENVIRONMENTAL FACTORS\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.1.1.5 ORGANIZATIONAL PROCESS ASSETS</td>\n",
       "      <td>11.1.1.5 ORGANIZATIONAL PROCESS ASSETS\\nThe or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sub_title  \\\n",
       "0                   11.1.1.1 PROJECT CHARTER   \n",
       "1           11.1.1.2 PROJECT MANAGEMENT PLAN   \n",
       "2                 11.1.1.3 PROJECT DOCUMENTS   \n",
       "3  11.1.1.4 ENTERPRISE ENVIRONMENTAL FACTORS   \n",
       "4     11.1.1.5 ORGANIZATIONAL PROCESS ASSETS   \n",
       "\n",
       "                                             contenu  \n",
       "0  11.1.1.1 PROJECT CHARTER\\nDescribed in Section...  \n",
       "1  11.1.1.2 PROJECT MANAGEMENT PLAN\\nDescribed in...  \n",
       "2  11.1.1.3 PROJECT DOCUMENTS\\nProject documents ...  \n",
       "3  11.1.1.4 ENTERPRISE ENVIRONMENTAL FACTORS\\nThe...  \n",
       "4  11.1.1.5 ORGANIZATIONAL PROCESS ASSETS\\nThe or...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_contenu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eecc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu.to_csv('sub_process.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2182898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of lemmatized define synonym\n",
    "dic=\"define definition setting identify defining set establishing classifying specifying means meaning\"\n",
    "doc = nlp(dic)\n",
    "liste=[]\n",
    "for token in doc:\n",
    "    liste.append(str(token.lemma_))\n",
    "definition_dic=list(set(liste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu['examples']=dataframe_contenu['contenu'].apply(lambda txt:Extract_example(txt))\n",
    "dataframe_contenu['definitions']=dataframe_contenu['contenu'].apply(lambda txt:extract_definition(definition_dic,txt))\n",
    "dataframe_contenu['actionable_items']=dataframe_contenu['contenu'].apply(lambda txt:extract_actionable_items(txt))\n",
    "dataframe_contenu['Extract_refenrence']=dataframe_contenu['contenu'].apply(lambda txt:Extract_refenrence(txt))\n",
    "dataframe_contenu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entites_with_relatonship\n",
    "dataframe_contenu['entites_relationship_predict']=dataframe_contenu['contenu'].apply(lambda txt:nlp_pipeline_entites_extraction(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "import pickle\n",
    "# bring annotated data\n",
    "file_path = 'evalution_metric.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    loaded_list = pickle.load(f)\n",
    "\n",
    "print(\"Loaded list:\", loaded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707900ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu['entites_relationship']=loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(true_entities, predicted_entities):\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = len(set(true_entities).intersection(predicted_entities))\n",
    "    false_positives = len(predicted_entities) - true_positives\n",
    "    false_negatives = len(true_entities) - true_positives\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0.0\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0.0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu[['precision', 'recall', 'f1_score']] = dataframe_contenu.apply(lambda row: calculate_scores(list(row['entites_relationship'].keys()), list(row['entites_relationship_predict'].keys())), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu['entites_relationship'][10].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decdc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu[['precision', 'recall', 'f1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate entites:\n",
    "\n",
    "average_precision = np.mean(dataframe_contenu['precision'])\n",
    "average_recall = np.mean(dataframe_contenu['recall'])\n",
    "average_f1_score = np.mean(dataframe_contenu['f1_score'])\n",
    "\n",
    "# Display the averages in a nicely formatted way\n",
    "print(\"Average Precision: {:.2f}\".format(average_precision))\n",
    "print(\"Average Recall: {:.2f}\".format(average_recall))\n",
    "print(\"Average F1 Score: {:.2f}\".format(average_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_contenu[['precision_relation', 'recall_relation', 'f1_score_relation']] = dataframe_contenu.apply(lambda row: calculate_scores(list(row['entites_relationship'].values()), list(row['entites_relationship_predict'].values())), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relationship metric\n",
    "average_precision = np.mean(dataframe_contenu['precision_relation'])\n",
    "average_recall = np.mean(dataframe_contenu['recall_relation'])\n",
    "average_f1_score = np.mean(dataframe_contenu['f1_score_relation'])\n",
    "\n",
    "# Display the averages in a nicely formatted way\n",
    "print(\"Average Precision: {:.2f}\".format(average_precision))\n",
    "print(\"Average Recall: {:.2f}\".format(average_recall))\n",
    "print(\"Average F1 Score: {:.2f}\".format(average_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703f0452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haboubi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bdcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
